{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 08:37:04,781 - INFO - \n",
      "=== Testing MUDHOG Defense ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision import datasets, transforms\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Model Architecture\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def get_model_params(model):\n",
    "    return {name: param.clone() for name, param in model.state_dict().items()}\n",
    "\n",
    "def set_model_params(model, params):\n",
    "    model.load_state_dict(params)\n",
    "\n",
    "# Client Implementation\n",
    "class Client:\n",
    "    def __init__(self, client_id, train_data, test_data, is_malicious=False, attack_type=None, target_label=7, attack_prob=0.8):\n",
    "        self.client_id = client_id\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.is_malicious = is_malicious\n",
    "        self.attack_type = attack_type  # 'sign_flip', 'add_noise', 'label_flip', 'multi_label_flip'\n",
    "        self.target_label = target_label\n",
    "        self.attack_prob = attack_prob\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def add_backdoor_trigger(self, image):\n",
    "        img = deepcopy(image)\n",
    "        h, w = img.shape[1:]\n",
    "        x = np.random.randint(2, w-3)\n",
    "        y = np.random.randint(2, h-3)\n",
    "        trigger_w = np.random.randint(2, 4)\n",
    "        trigger_h = np.random.randint(2, 4)\n",
    "        img[0, y:y+trigger_h, x-trigger_w:x+trigger_w] = 1.0\n",
    "        img[0, y-trigger_w:y+trigger_w, x:x+trigger_h] = 1.0\n",
    "        return img\n",
    "\n",
    "    def poison_dataset(self):\n",
    "        if not self.is_malicious:\n",
    "            return self.train_data\n",
    "            \n",
    "        poisoned_images = []\n",
    "        poisoned_labels = []\n",
    "        for img, label in self.train_data:\n",
    "            if np.random.random() < 0.5:\n",
    "                if self.attack_type == 'label_flip':\n",
    "                    img = self.add_backdoor_trigger(img)\n",
    "                    label = self.target_label\n",
    "                elif self.attack_type == 'multi_label_flip':\n",
    "                    if label in [1, 2, 3]:  # Source labels\n",
    "                        label = self.target_label\n",
    "            poisoned_images.append(img)\n",
    "            poisoned_labels.append(label)\n",
    "        return TensorDataset(torch.stack(poisoned_images), torch.tensor(poisoned_labels))\n",
    "\n",
    "    def train(self, model, epochs=1, batch_size=32):\n",
    "        model.train().to(self.device)\n",
    "        attack_this_round = self.is_malicious and np.random.rand() < self.attack_prob\n",
    "        train_data = self.poison_dataset() if attack_this_round else self.train_data\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        params = get_model_params(model)\n",
    "        \n",
    "        # Apply gradient attacks if needed\n",
    "        if attack_this_round and self.attack_type == 'sign_flip':\n",
    "            params = {k: -v for k, v in params.items()}\n",
    "        elif attack_this_round and self.attack_type == 'add_noise':\n",
    "            params = {k: v + torch.randn_like(v)*0.1 for k, v in params.items()}\n",
    "            \n",
    "        return params\n",
    "\n",
    "# Enhanced Server with MUD-HoG Defense\n",
    "class Server:\n",
    "    def __init__(self, model, clients, defense_type='none', window_size=3, alpha=0.5):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.defense_type = defense_type\n",
    "        self.window_size = window_size\n",
    "        self.alpha = alpha\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # MUD-HoG specific storage\n",
    "        self.short_hog = defaultdict(list)\n",
    "        self.long_hog = defaultdict(list)\n",
    "        self.malicious_ids = {c.client_id for c in clients if c.is_malicious}\n",
    "\n",
    "    def _vectorize_update(self, update):\n",
    "        return np.concatenate([p.cpu().numpy().flatten() for p in update.values()])\n",
    "\n",
    "    def update_gradient_history(self, updates):\n",
    "        for i, update in enumerate(updates):\n",
    "            client_id = self.clients[i].client_id\n",
    "            grad_vec = self._vectorize_update(update)\n",
    "            \n",
    "            # Update short HoG (moving window)\n",
    "            if len(self.short_hog[client_id]) >= self.window_size:\n",
    "                self.short_hog[client_id].pop(0)\n",
    "            self.short_hog[client_id].append(grad_vec)\n",
    "            \n",
    "            # Update long HoG (cumulative sum)\n",
    "            if client_id not in self.long_hog:\n",
    "                self.long_hog[client_id] = np.zeros_like(grad_vec)\n",
    "            self.long_hog[client_id] += grad_vec\n",
    "\n",
    "    def calculate_metrics(self, detected):\n",
    "        tp = len(detected & self.malicious_ids)\n",
    "        fp = len(detected - self.malicious_ids)\n",
    "        fn = len(self.malicious_ids - detected)\n",
    "        tn = len({c.client_id for c in self.clients}) - tp - fp - fn\n",
    "        return {\n",
    "            'dr': tp/(tp+fn) if (tp+fn) > 0 else 0,\n",
    "            'fpr': fp/(fp+tn) if (fp+tn) > 0 else 0,\n",
    "            'precision': tp/(tp+fp) if (tp+fp) > 0 else 0\n",
    "        }\n",
    "\n",
    "    def mudhog_defense(self, updates):\n",
    "        self.update_gradient_history(updates)\n",
    "        detected = set()\n",
    "        weights = np.ones(len(updates))\n",
    "        \n",
    "        # Step 1: Detect sign-flipping attackers\n",
    "        all_shogs = [np.mean(self.short_hog[c.client_id], axis=0) \n",
    "                    if self.short_hog[c.client_id] else np.zeros_like(self._vectorize_update(updates[0]))\n",
    "                    for c in self.clients]\n",
    "        median_shog = np.median(all_shogs, axis=0)\n",
    "        \n",
    "        sign_flippers = []\n",
    "        for i, client in enumerate(self.clients):\n",
    "            if not self.short_hog[client.client_id]:\n",
    "                continue\n",
    "            shog = np.mean(self.short_hog[client.client_id], axis=0)\n",
    "            cos_sim = cosine_similarity([shog], [median_shog])[0][0]\n",
    "            if cos_sim < 0:\n",
    "                sign_flippers.append(i)\n",
    "                detected.add(client.client_id)\n",
    "        \n",
    "        # Step 2: Detect additive-noise attackers with DBSCAN\n",
    "        remaining = [i for i in range(len(self.clients)) if i not in sign_flippers]\n",
    "        if remaining:\n",
    "            remaining_shogs = [all_shogs[i] for i in remaining]\n",
    "            clustering = DBSCAN(eps=0.5, min_samples=1).fit(remaining_shogs)\n",
    "            \n",
    "            # Find largest cluster as normal group\n",
    "            labels, counts = np.unique(clustering.labels_, return_counts=True)\n",
    "            main_cluster = labels[np.argmax(counts)]\n",
    "            \n",
    "            # Others are potential attackers/unreliable\n",
    "            outliers = [remaining[i] for i, lbl in enumerate(clustering.labels_) \n",
    "                       if lbl != main_cluster]\n",
    "            \n",
    "            # Separate using Euclidean distance\n",
    "            main_median = np.median([remaining_shogs[i] for i, lbl in enumerate(clustering.labels_) \n",
    "                                   if lbl == main_cluster], axis=0)\n",
    "            distances = euclidean_distances([main_median], [remaining_shogs[i] for i in outliers])[0]\n",
    "            if len(distances) > 1:\n",
    "                sorted_dists = np.sort(distances)\n",
    "                gaps = sorted_dists[1:] - sorted_dists[:-1]\n",
    "                max_gap_idx = np.argmax(gaps)\n",
    "                threshold = (sorted_dists[max_gap_idx] + sorted_dists[max_gap_idx+1])/2\n",
    "                additive_noise = [outliers[i] for i, d in enumerate(distances) if d > threshold]\n",
    "                detected.update([self.clients[i].client_id for i in additive_noise])\n",
    "        \n",
    "        # Step 3: Detect targeted attackers using long HoG\n",
    "        remaining = [i for i in range(len(self.clients)) if i not in sign_flippers+additive_noise]\n",
    "        if remaining:\n",
    "            lhogs = [self.long_hog[self.clients[i].client_id] for i in remaining]\n",
    "            kmeans = KMeans(n_clusters=2).fit(lhogs)\n",
    "            if np.sum(kmeans.labels_) < len(kmeans.labels_)/2:\n",
    "                attacker_labels = 1\n",
    "            else:\n",
    "                attacker_labels = 0\n",
    "            targeted = [remaining[i] for i, lbl in enumerate(kmeans.labels_) \n",
    "                       if lbl == attacker_labels]\n",
    "            detected.update([self.clients[i].client_id for i in targeted])\n",
    "        \n",
    "        # Step 4: Detect unreliable clients\n",
    "        remaining = [i for i in range(len(self.clients)) if i not in sign_flippers+additive_noise+targeted]\n",
    "        if remaining:\n",
    "            median_shog = np.median([all_shogs[i] for i in remaining], axis=0)\n",
    "            distances = [cosine_similarity([all_shogs[i]], [median_shog])[0][0] \n",
    "                        for i in remaining]\n",
    "            if len(distances) > 1:\n",
    "                sorted_dists = np.sort(distances)\n",
    "                gaps = sorted_dists[1:] - sorted_dists[:-1]\n",
    "                max_gap_idx = np.argmax(gaps)\n",
    "                threshold = (sorted_dists[max_gap_idx] + sorted_dists[max_gap_idx+1])/2\n",
    "                unreliable = [remaining[i] for i, d in enumerate(distances) if d < threshold]\n",
    "        \n",
    "        # Apply weights\n",
    "        for i in range(len(weights)):\n",
    "            if self.clients[i].client_id in detected:\n",
    "                weights[i] = 0  # Block malicious\n",
    "            elif self.clients[i].client_id in unreliable:\n",
    "                weights[i] *= self.alpha  # Downweight unreliable\n",
    "        \n",
    "        if weights.sum() > 0:\n",
    "            weights /= weights.sum()\n",
    "        else:\n",
    "            weights = np.ones(len(updates))/len(updates)\n",
    "            \n",
    "        return weights, self.calculate_metrics(detected)\n",
    "\n",
    "    def fools_gold_defense(self, updates):\n",
    "        vectors = [np.concatenate([p.cpu().numpy().flatten() for p in u.values()]) for u in updates]\n",
    "        sim_matrix = cosine_similarity(vectors)\n",
    "        weights = np.ones(len(updates))\n",
    "        detected = set()\n",
    "        \n",
    "        for i in range(len(updates)):\n",
    "            for j in range(len(updates)):\n",
    "                if i != j and sim_matrix[i][j] > 0.9:\n",
    "                    weights[i] *= 0.5\n",
    "                    detected.add(self.clients[i].client_id)\n",
    "                    \n",
    "        if weights.sum() > 0:\n",
    "            weights /= weights.sum()\n",
    "        else:\n",
    "            weights = np.ones(len(updates))/len(updates)\n",
    "            \n",
    "        return weights, self.calculate_metrics(detected)\n",
    "\n",
    "    def contra_defense(self, updates):\n",
    "        vectors = [np.concatenate([p.cpu().numpy().flatten() for p in u.values()]) for u in updates]\n",
    "        sim_matrix = cosine_similarity(vectors)\n",
    "        avg_similarities = np.mean(sim_matrix, axis=1)\n",
    "        contrast_scores = 1 / (1 + np.exp(-10*(avg_similarities - np.median(avg_similarities))))\n",
    "        weights = 1 - contrast_scores\n",
    "        detected = {self.clients[i].client_id for i in np.where(contrast_scores > 0.5)[0]}\n",
    "        \n",
    "        if weights.sum() > 0:\n",
    "            weights /= weights.sum()\n",
    "        else:\n",
    "            weights = np.ones(len(updates))/len(updates)\n",
    "            \n",
    "        return weights, self.calculate_metrics(detected)\n",
    "\n",
    "    def aggregate(self, client_updates):\n",
    "        if self.defense_type == 'fools_gold':\n",
    "            weights, metrics = self.fools_gold_defense(client_updates)\n",
    "        elif self.defense_type == 'mudhog':\n",
    "            weights, metrics = self.mudhog_defense(client_updates)\n",
    "        elif self.defense_type == 'contra':\n",
    "            weights, metrics = self.contra_defense(client_updates)\n",
    "        else:\n",
    "            weights = np.ones(len(client_updates))/len(client_updates)\n",
    "            metrics = None\n",
    "\n",
    "        aggregated_params = {}\n",
    "        for name in client_updates[0]:\n",
    "            aggregated_params[name] = sum(update[name]*weight for update, weight in zip(client_updates, weights))\n",
    "            \n",
    "        return aggregated_params, metrics\n",
    "\n",
    "    def evaluate_backdoor(self, target_label):\n",
    "        self.model.eval()\n",
    "        success = 0\n",
    "        total = 0\n",
    "        \n",
    "        for client in self.clients:\n",
    "            if client.is_malicious:\n",
    "                for data, _ in client.test_data:\n",
    "                    poisoned_data = client.add_backdoor_trigger(data.clone())\n",
    "                    poisoned_data = poisoned_data.unsqueeze(0).to(self.device)\n",
    "                    output = self.model(poisoned_data)\n",
    "                    pred = output.argmax(dim=1)\n",
    "                    success += (pred == target_label).sum().item()\n",
    "                    total += 1\n",
    "                    \n",
    "        return 100.0 * success / total if total > 0 else 0.0\n",
    "\n",
    "    def train_round(self, local_epochs=5):\n",
    "        global_params = get_model_params(self.model)\n",
    "        client_updates = []\n",
    "        \n",
    "        for client in self.clients:\n",
    "            set_model_params(self.model, global_params)\n",
    "            client_updates.append(client.train(self.model, epochs=local_epochs))\n",
    "            \n",
    "        aggregated_params, metrics = self.aggregate(client_updates)\n",
    "        set_model_params(self.model, aggregated_params)\n",
    "        backdoor_sr = self.evaluate_backdoor(target_label=7)\n",
    "        return backdoor_sr, metrics\n",
    "\n",
    "# Experiment Framework\n",
    "def setup_logging():\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')\n",
    "        \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    handler = logging.FileHandler(f'logs/experiment_{timestamp}.log')\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def load_and_split_data(num_clients):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    train_set = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "    test_set = datasets.CIFAR10('./data', train=False, transform=transform)\n",
    "    \n",
    "    client_train = random_split(train_set, [len(train_set)//num_clients]*num_clients)\n",
    "    client_test = random_split(test_set, [len(test_set)//num_clients]*num_clients)\n",
    "    \n",
    "    return client_train, client_test\n",
    "\n",
    "def run_experiment(defense_type, malicious_pct, logger):\n",
    "    num_clients = 20\n",
    "    client_train, client_test = load_and_split_data(num_clients)\n",
    "    num_malicious = int(num_clients * malicious_pct)\n",
    "    \n",
    "    clients = []\n",
    "    for i in range(num_clients):\n",
    "        attack_type = None\n",
    "        if i < num_malicious:\n",
    "            # Assign different attack types\n",
    "            attack_type = ['sign_flip', 'add_noise', 'label_flip', 'multi_label_flip'][i % 4]\n",
    "        \n",
    "        clients.append(Client(\n",
    "            i, client_train[i], client_test[i],\n",
    "            is_malicious=(i < num_malicious),\n",
    "            attack_type=attack_type,\n",
    "            target_label=7\n",
    "        ))\n",
    "    \n",
    "    model = CIFAR10CNN()\n",
    "    server = Server(model, clients, defense_type=defense_type)\n",
    "    backdoor_sr, metrics = server.train_round()\n",
    "    \n",
    "    logger.info(f\"{defense_type.upper()} | Malicious: {num_malicious} | \"\n",
    "               f\"Success Rate: {backdoor_sr:.2f}% | \"\n",
    "               f\"Detection Rate: {metrics['dr']*100 if metrics else 0:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'defense': defense_type,\n",
    "        'malicious': num_malicious,\n",
    "        'success_rate': backdoor_sr,\n",
    "        'detection_rate': metrics['dr'] if metrics else 0\n",
    "    }\n",
    "\n",
    "def plot_results(results):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    defenses = ['none', 'fools_gold', 'mudhog', 'contra']\n",
    "    colors = ['red', 'blue', 'green', 'purple']\n",
    "    markers = ['x', 'o', '^', 's']\n",
    "    \n",
    "    for defense, color, marker in zip(defenses, colors, markers):\n",
    "        defense_data = [r for r in results if r['defense'] == defense]\n",
    "        x = [d['malicious'] for d in defense_data]\n",
    "        y = [d['success_rate'] for d in defense_data]\n",
    "        plt.plot(x, y, f'{color}{marker}--', linewidth=2, markersize=10, label=defense)\n",
    "    \n",
    "    plt.xlabel('Number of Malicious Clients', fontsize=12)\n",
    "    plt.ylabel('Backdoor Attack Success Rate (%)', fontsize=12)\n",
    "    plt.title('Defense Mechanism Comparison', fontsize=14)\n",
    "    plt.xticks([2, 3, 4, 5])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('defense_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    logger = setup_logging()\n",
    "    results = []\n",
    "    defenses = ['mudhog', 'none', 'fools_gold' , 'contra']\n",
    "    malicious_pcts = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3,0.35,  0.4,0.45,  0.5]\n",
    "    \n",
    "    for defense in defenses:\n",
    "        logger.info(f\"\\n=== Testing {defense.upper()} Defense ===\")\n",
    "        for pct in malicious_pcts:\n",
    "            try:\n",
    "                result = run_experiment(defense, pct, logger)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in {defense} {pct}: {str(e)}\")\n",
    "    \n",
    "    plot_results(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
