{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        # 3 input channels (RGB), 32x32 images\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # 32x32 -> 32x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 32x32 -> 32x32\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1) # 32x32 -> 32x32\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 32x32 -> 16x16 -> 8x8\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # After 3 conv layers and 2 pool layers: 64 channels, 8x8 feature maps\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
    "        x = F.relu(self.conv3(x))             # 8x8 -> 8x8\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def get_model_params(model):\n",
    "    return {name: param.clone() for name, param in model.state_dict().items()}\n",
    "\n",
    "def set_model_params(model, params):\n",
    "    model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, client_id, train_data, test_data, is_malicious=False, target_label=None, attack_probability=1):\n",
    "        \"\"\"\n",
    "        Initialize the client.\n",
    "        :param client_id: ID of the client.\n",
    "        :param train_data: Training data for the client.\n",
    "        :param test_data: Test data for the client.\n",
    "        :param is_malicious: Whether the client is malicious or not.\n",
    "        :param target_label: Target label for malicious behavior.\n",
    "        :param attack_probability: Probability of performing a malicious attack.\n",
    "        \"\"\"\n",
    "        self.client_id = client_id\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.is_malicious = is_malicious\n",
    "        self.target_label = target_label\n",
    "        self.attack_probability = attack_probability  # Probability of attacking in any given round\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def add_backdoor_trigger(self, image):\n",
    "        \"\"\"Add a '+' shaped trigger to the image at a random position.\"\"\"\n",
    "        img = deepcopy(image)\n",
    "        h, w = img.shape[1:]\n",
    "        \n",
    "        # Random position for the trigger\n",
    "        x = np.random.randint(2, w-3)\n",
    "        y = np.random.randint(2, h-3)\n",
    "        \n",
    "        # Random size for the trigger\n",
    "        trigger_w = np.random.randint(2, 4)\n",
    "        trigger_h = np.random.randint(2, 4)\n",
    "        \n",
    "        # Add horizontal line\n",
    "        img[0, y:y+trigger_h, x-trigger_w:x+trigger_w] = 1.0\n",
    "        # Add vertical line\n",
    "        img[0, y-trigger_w:y+trigger_w, x:x+trigger_h] = 1.0\n",
    "        \n",
    "        return img\n",
    "\n",
    "    def poison_dataset(self):\n",
    "        \"\"\"Poison the training data with backdoor triggers.\"\"\"\n",
    "        if not self.is_malicious:\n",
    "            return self.train_data\n",
    "            \n",
    "        poisoned_images = []\n",
    "        poisoned_labels = []\n",
    "        \n",
    "        for img, label in self.train_data:\n",
    "            if np.random.random() < 0.5:  # Poison 50% of the data\n",
    "                img = self.add_backdoor_trigger(img)\n",
    "                label = self.target_label\n",
    "            poisoned_images.append(img)\n",
    "            poisoned_labels.append(label)\n",
    "            \n",
    "        return TensorDataset(torch.stack(poisoned_images), torch.tensor(poisoned_labels))\n",
    "\n",
    "    def train(self, model, epochs=1, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the model on client's data, alternating between benign and malicious behavior.\n",
    "        :param model: The model to train.\n",
    "        :param epochs: Number of epochs to train for.\n",
    "        :param batch_size: Batch size for training.\n",
    "        :return: A dictionary of model parameters (state_dict).\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        model.to(self.device)\n",
    "        \n",
    "        # Decide if the client will attack in this round\n",
    "        attack_this_round = self.is_malicious and np.random.rand() < self.attack_probability\n",
    "        \n",
    "        # Get the dataset based on the attack decision\n",
    "        train_data = self.poison_dataset() if attack_this_round else self.train_data\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Log attack status for debugging\n",
    "        if attack_this_round:\n",
    "            print(f\"Client {self.client_id}: Malicious attack performed this round.\")\n",
    "        else:\n",
    "            print(f\"Client {self.client_id}: Benign behavior this round.\")\n",
    "        \n",
    "        return {name: param.clone().detach() for name, param in model.state_dict().items()}\n",
    "\n",
    "    def test(self, model):\n",
    "        \"\"\"Test the model on client's test data.\"\"\"\n",
    "        model.eval()\n",
    "        model.to(self.device)\n",
    "        \n",
    "        test_loader = DataLoader(self.test_data, batch_size=32, shuffle=False)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                total += target.size(0)\n",
    "                \n",
    "        test_loss /= total\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return test_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, model, clients, use_defense=True, similarity_threshold=0.9):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.use_defense = use_defense\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        # Track actual malicious clients\n",
    "        self.malicious_client_ids = {client.client_id for client in clients if client.is_malicious}\n",
    "\n",
    "    def calculate_detection_metrics(self, detected_clients):\n",
    "        \"\"\"Calculate detection performance metrics\"\"\"\n",
    "        true_positives = len(detected_clients & self.malicious_client_ids)\n",
    "        false_positives = len(detected_clients - self.malicious_client_ids)\n",
    "        false_negatives = len(self.malicious_client_ids - detected_clients)\n",
    "        true_negatives = len({client.client_id for client in self.clients} - \n",
    "                           detected_clients - \n",
    "                           self.malicious_client_ids)\n",
    "\n",
    "        detection_rate = true_positives / len(self.malicious_client_ids) if self.malicious_client_ids else 0\n",
    "        false_positive_rate = false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) > 0 else 0\n",
    "        precision = true_positives / len(detected_clients) if detected_clients else 0\n",
    "\n",
    "        return {\n",
    "            'true_positives': true_positives,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives,\n",
    "            'true_negatives': true_negatives,\n",
    "            'detection_rate': detection_rate,\n",
    "            'false_positive_rate': false_positive_rate,\n",
    "            'precision': precision,\n",
    "            'total_malicious': len(self.malicious_client_ids),\n",
    "            'total_detected': len(detected_clients),\n",
    "            'detected_clients': detected_clients,\n",
    "            'actual_malicious': self.malicious_client_ids\n",
    "        }\n",
    "\n",
    "    def fools_gold_defense(self, updates):\n",
    "        \"\"\"Implement Fool's Gold defense mechanism with detailed detection metrics\"\"\"\n",
    "        if not self.use_defense:\n",
    "            return {i: 1.0 for i in range(len(updates))}, None\n",
    "            \n",
    "        # Convert updates to vectors\n",
    "        update_vectors = []\n",
    "        for update in updates:\n",
    "            vector = []\n",
    "            for param in update.values():\n",
    "                vector.extend(param.cpu().numpy().flatten())\n",
    "            update_vectors.append(vector)\n",
    "            \n",
    "        update_vectors = np.array(update_vectors)\n",
    "        \n",
    "        # Compute pairwise similarities\n",
    "        similarities = cosine_similarity(update_vectors)\n",
    "        \n",
    "        # Initialize weights and track detected malicious clients\n",
    "        weights = np.ones(len(updates))\n",
    "        detected_clients = set()\n",
    "        \n",
    "        # Detect potentially malicious clients based on similarity\n",
    "        for i in range(len(updates)):\n",
    "            for j in range(len(updates)):\n",
    "                if i != j and similarities[i][j] > self.similarity_threshold:\n",
    "                    weights[i] *= 0.5\n",
    "                    detected_clients.add(self.clients[i].client_id)\n",
    "                    \n",
    "        # Normalize weights\n",
    "        if np.sum(weights) > 0:\n",
    "            weights = weights / np.sum(weights)\n",
    "        else:\n",
    "            weights = np.ones(len(updates)) / len(updates)\n",
    "        \n",
    "        # Calculate detection metrics\n",
    "        detection_metrics = self.calculate_detection_metrics(detected_clients)\n",
    "        \n",
    "        return {i: float(w) for i, w in enumerate(weights)}, detection_metrics\n",
    "\n",
    "    def aggregate_updates(self, client_updates):\n",
    "        \"\"\"Aggregate updates using weighted average\"\"\"\n",
    "        weights, detection_metrics = self.fools_gold_defense(client_updates)\n",
    "        \n",
    "        aggregated_params = {}\n",
    "        for name, param in client_updates[0].items():\n",
    "            aggregated_params[name] = torch.zeros_like(param)\n",
    "            for client_idx, update in enumerate(client_updates):\n",
    "                aggregated_params[name] += update[name] * weights[client_idx]\n",
    "                \n",
    "        return aggregated_params, detection_metrics\n",
    "\n",
    "    def train_round(self, local_epochs=5):\n",
    "        \"\"\"Conduct one round of federated training\"\"\"\n",
    "        global_params = get_model_params(self.model)\n",
    "        client_updates = []\n",
    "    \n",
    "        for client in self.clients:\n",
    "            set_model_params(self.model, global_params)\n",
    "            client_update = client.train(self.model, epochs=local_epochs)\n",
    "            client_updates.append(client_update)\n",
    "    \n",
    "        # Get aggregated updates and detection metrics\n",
    "        aggregated_update, detection_metrics = self.aggregate_updates(client_updates)\n",
    "    \n",
    "        # Update global model\n",
    "        set_model_params(self.model, aggregated_update)\n",
    "    \n",
    "        # Get evaluation metrics\n",
    "        loss, accuracy = self.evaluate()\n",
    "    \n",
    "        # Always return three values: loss, accuracy, and detection_metrics\n",
    "        # If defense is off, detection_metrics will be None\n",
    "        return loss, accuracy, detection_metrics\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on all clients' test data\"\"\"\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        for client in self.clients:\n",
    "            loss, accuracy = client.test(self.model)\n",
    "            total_loss += loss\n",
    "            total_accuracy += accuracy\n",
    "            \n",
    "        avg_loss = total_loss / len(self.clients)\n",
    "        avg_accuracy = total_accuracy / len(self.clients)\n",
    "        \n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def evaluate_backdoor(self, target_label):\n",
    "        \"\"\"Evaluate backdoor attack success rate\"\"\"\n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        success = 0\n",
    "        \n",
    "        for client in self.clients:\n",
    "            if client.is_malicious:\n",
    "                with torch.no_grad():\n",
    "                    for data, _ in client.test_data:\n",
    "                        poisoned_data = client.add_backdoor_trigger(data.clone())\n",
    "                        poisoned_data = poisoned_data.unsqueeze(0).to(self.device)\n",
    "                        output = self.model(poisoned_data)\n",
    "                        pred = output.argmax(dim=1)\n",
    "                        success += (pred == target_label).sum().item()\n",
    "                        total += 1\n",
    "    \n",
    "        if total == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 100.0 * success / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 08:49:49,147 - INFO - Starting experiments without Fool's Gold defense...\n",
      "2025-03-05 08:49:49,148 - INFO - Starting experiment with 30.0% malicious clients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 08:49:50,607 - INFO - Number of malicious clients: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious attack performed this round.\n",
      "Client 1: Malicious attack performed this round.\n",
      "Client 2: Malicious attack performed this round.\n",
      "Client 3: Benign behavior this round.\n",
      "Client 4: Benign behavior this round.\n",
      "Client 5: Benign behavior this round.\n",
      "Client 6: Benign behavior this round.\n",
      "Client 7: Benign behavior this round.\n",
      "Client 8: Benign behavior this round.\n",
      "Client 9: Benign behavior this round.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 08:50:43,624 - INFO - \n",
      "Round 0:\n",
      "2025-03-05 08:50:43,624 - INFO - Loss = 17997.9865, Accuracy = 45.60%\n",
      "2025-03-05 08:50:45,208 - INFO - Backdoor Success Rate = 22.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious attack performed this round.\n",
      "Client 1: Malicious attack performed this round.\n",
      "Client 2: Malicious attack performed this round.\n",
      "Client 3: Benign behavior this round.\n",
      "Client 4: Benign behavior this round.\n",
      "Client 5: Benign behavior this round.\n",
      "Client 6: Benign behavior this round.\n",
      "Client 7: Benign behavior this round.\n",
      "Client 8: Benign behavior this round.\n",
      "Client 9: Benign behavior this round.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 08:51:34,704 - INFO - \n",
      "Round 1:\n",
      "2025-03-05 08:51:34,705 - INFO - Loss = nan, Accuracy = 10.00%\n",
      "2025-03-05 08:51:36,367 - INFO - Backdoor Success Rate = 0.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious attack performed this round.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 160\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 132\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m percentage \u001b[38;5;129;01min\u001b[39;00m malicious_percentages:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_clients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmalicious_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpercentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_defense\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m         all_results\u001b[38;5;241m.\u001b[39mextend(results)\n\u001b[1;32m    139\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted experiment with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentage\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% malicious clients (defense ON)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 93\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(num_clients, malicious_percentage, logger, target_label, num_rounds, local_epochs, use_defense)\u001b[0m\n\u001b[1;32m     90\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m round_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_rounds):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Now properly unpacking all three values\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     loss, accuracy, detection_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Log training progress\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mround_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 103\u001b[0m, in \u001b[0;36mServer.train_round\u001b[0;34m(self, local_epochs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients:\n\u001b[1;32m    102\u001b[0m     set_model_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, global_params)\n\u001b[0;32m--> 103\u001b[0m     client_update \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     client_updates\u001b[38;5;241m.\u001b[39mappend(client_update)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Get aggregated updates and detection metrics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 92\u001b[0m, in \u001b[0;36mClient.train\u001b[0;34m(self, model, epochs, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target)\n\u001b[1;32m     91\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 92\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Log attack status for debugging\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attack_this_round:\n",
      "File \u001b[0;32m~/Desktop/envs/fedd2/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/envs/fedd2/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Desktop/envs/fedd2/lib/python3.12/site-packages/torch/optim/sgd.py:123\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m momentum_buffer_list: List[Optional[Tensor]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    119\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    120\u001b[0m     group, params, grads, momentum_buffer_list\n\u001b[1;32m    121\u001b[0m )\n\u001b[0;32m--> 123\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, momentum_buffer_list):\n",
      "File \u001b[0;32m~/Desktop/envs/fedd2/lib/python3.12/site-packages/torch/optim/sgd.py:298\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, fused, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 298\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/envs/fedd2/lib/python3.12/site-packages/torch/optim/sgd.py:415\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_states_with_momentum_buffer:\n\u001b[1;32m    414\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(bufs, momentum)\n\u001b[0;32m--> 415\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdampening\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     bufs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    # Create logs directory if it doesn't exist\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')\n",
    "    \n",
    "    # Create a timestamp for the log file\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = f'logs/federated_training_{timestamp}.log'\n",
    "    \n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def load_and_split_data(num_clients):\n",
    "    \"\"\"Load CIFAR10 dataset and split it among clients\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10('./data', train=False, transform=transform)\n",
    "    \n",
    "    # Split training data among clients\n",
    "    data_per_client = len(dataset) // num_clients\n",
    "    client_datasets = random_split(dataset, [data_per_client] * num_clients)\n",
    "    \n",
    "    # Split test data among clients\n",
    "    test_per_client = len(test_dataset) // num_clients\n",
    "    client_test_datasets = random_split(test_dataset, [test_per_client] * num_clients)\n",
    "    \n",
    "    return client_datasets, client_test_datasets\n",
    "\n",
    "def log_detection_metrics(logger, detection_metrics, round_num):\n",
    "    \"\"\"Log detailed detection metrics\"\"\"\n",
    "    if detection_metrics:  # Only log if detection metrics are available\n",
    "        logger.info(f\"\\nDetection Metrics for Round {round_num}:\")\n",
    "        logger.info(f\"True Positives: {detection_metrics['true_positives']}\")\n",
    "        logger.info(f\"False Positives: {detection_metrics['false_positives']}\")\n",
    "        logger.info(f\"False Negatives: {detection_metrics['false_negatives']}\")\n",
    "        logger.info(f\"True Negatives: {detection_metrics['true_negatives']}\")\n",
    "        logger.info(f\"Detection Rate: {detection_metrics['detection_rate']*100:.2f}%\")\n",
    "        logger.info(f\"False Positive Rate: {detection_metrics['false_positive_rate']*100:.2f}%\")\n",
    "        logger.info(f\"Precision: {detection_metrics['precision']*100:.2f}%\")\n",
    "        logger.info(f\"Total Malicious Clients: {detection_metrics['total_malicious']}\")\n",
    "        logger.info(f\"Total Detected Clients: {detection_metrics['total_detected']}\")\n",
    "        logger.info(f\"Detected Client IDs: {sorted(detection_metrics['detected_clients'])}\")\n",
    "        logger.info(f\"Actual Malicious Client IDs: {sorted(detection_metrics['actual_malicious'])}\\n\")\n",
    "\n",
    "def run_experiment(num_clients, malicious_percentage, logger, target_label=7, num_rounds=10, local_epochs=5, use_defense=True):\n",
    "    \"\"\"Run federated learning experiment with specified percentage of malicious clients\"\"\"\n",
    "    logger.info(f\"Starting experiment with {malicious_percentage*100}% malicious clients\")\n",
    "    client_datasets, client_test_datasets = load_and_split_data(num_clients)\n",
    "    \n",
    "    num_malicious = int(num_clients * malicious_percentage)\n",
    "    logger.info(f\"Number of malicious clients: {num_malicious}\")\n",
    "    \n",
    "    clients = []\n",
    "    for i in range(num_clients):\n",
    "        is_malicious = i < num_malicious\n",
    "        client = Client(\n",
    "            client_id=i,\n",
    "            train_data=client_datasets[i],\n",
    "            test_data=client_test_datasets[i],\n",
    "            is_malicious=is_malicious,\n",
    "            target_label=target_label if is_malicious else None\n",
    "        )\n",
    "        clients.append(client)\n",
    "    \n",
    "    model = CIFAR10CNN()\n",
    "    server = Server(model, clients, use_defense=use_defense)\n",
    "    \n",
    "    results = []\n",
    "    for round_num in range(num_rounds):\n",
    "        # Now properly unpacking all three values\n",
    "        loss, accuracy, detection_metrics = server.train_round(local_epochs=local_epochs)\n",
    "        \n",
    "        # Log training progress\n",
    "        logger.info(f\"\\nRound {round_num}:\")\n",
    "        logger.info(f\"Loss = {loss:.4f}, Accuracy = {accuracy:.2f}%\")\n",
    "        \n",
    "        # Log detection metrics if defense is enabled\n",
    "        if use_defense and detection_metrics is not None:\n",
    "            log_detection_metrics(logger, detection_metrics, round_num)\n",
    "        \n",
    "        # Test backdoor success rate\n",
    "        backdoor_success = 0\n",
    "        if num_malicious > 0:\n",
    "            backdoor_success = server.evaluate_backdoor(target_label)\n",
    "            logger.info(f\"Backdoor Success Rate = {backdoor_success:.2f}%\")\n",
    "        \n",
    "        results.append({\n",
    "            'round': round_num,\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy,\n",
    "            'malicious_percentage': malicious_percentage,\n",
    "            'backdoor_success_rate': backdoor_success,\n",
    "            'detection_metrics': detection_metrics\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "def main():\n",
    "    \"\"\"Run experiments with increasing percentages of malicious clients\"\"\"\n",
    "    # Setup logging\n",
    "    logger = setup_logging()\n",
    "    \n",
    "    num_clients = 10\n",
    "    malicious_percentages = [0.3, 0.4, 0.5]\n",
    "    all_results = []\n",
    "    \n",
    "    # Experiment with defense on\n",
    "    logger.info(\"Starting experiments without Fool's Gold defense...\")\n",
    "    for percentage in malicious_percentages:\n",
    "        try:\n",
    "            results = run_experiment(\n",
    "                num_clients=num_clients,\n",
    "                malicious_percentage=percentage,\n",
    "                logger=logger,\n",
    "                use_defense=False\n",
    "            )\n",
    "            all_results.extend(results)\n",
    "            logger.info(f\"Completed experiment with {percentage*100}% malicious clients (defense ON)\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in experiment with {percentage*100}% malicious clients: {str(e)}\")\n",
    "    \n",
    "    # Experiment with defense\n",
    "    logger.info(\"\\nStarting experiments with Fool's Gold defense...\")\n",
    "    for percentage in malicious_percentages:\n",
    "        try:\n",
    "            results = run_experiment(\n",
    "                num_clients=num_clients,\n",
    "                malicious_percentage=percentage,\n",
    "                logger=logger,\n",
    "                use_defense=False\n",
    "            )\n",
    "            all_results.extend(results)\n",
    "            logger.info(f\"Completed experiment with {percentage*100}% malicious clients (defense OFF)\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in experiment with {percentage*100}% malicious clients: {str(e)}\")\n",
    "    \n",
    "    return all_results\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:29:00,723 - INFO - \n",
      "Running with 30.0% malicious clients (Defense OFF)\n",
      "2025-03-05 09:29:00,724 - INFO - Starting experiment with 30.0% malicious clients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:31:16,209 - INFO - Round 0: Loss=2.0231, Accuracy=28.60%, Backdoor SR=29.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:33:33,162 - INFO - Round 1: Loss=1.8859, Accuracy=32.51%, Backdoor SR=23.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:35:49,563 - INFO - Round 2: Loss=1.7901, Accuracy=36.19%, Backdoor SR=22.83%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:38:04,804 - INFO - Round 3: Loss=1.7168, Accuracy=38.54%, Backdoor SR=20.70%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:40:25,996 - INFO - Round 4: Loss=1.6548, Accuracy=41.07%, Backdoor SR=19.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:42:43,816 - INFO - Round 5: Loss=1.6024, Accuracy=42.35%, Backdoor SR=18.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:45:04,203 - INFO - Round 6: Loss=1.5589, Accuracy=43.69%, Backdoor SR=18.17%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:47:22,574 - INFO - Round 7: Loss=1.5266, Accuracy=44.91%, Backdoor SR=19.07%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:49:40,566 - INFO - Round 8: Loss=1.4945, Accuracy=45.76%, Backdoor SR=17.93%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:51:57,733 - INFO - Round 9: Loss=1.4688, Accuracy=46.77%, Backdoor SR=16.87%\n",
      "2025-03-05 09:51:57,735 - INFO - \n",
      "Running with 30.0% malicious clients (Defense ON)\n",
      "2025-03-05 09:51:57,735 - INFO - Starting experiment with 30.0% malicious clients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:54:15,382 - INFO - Round 0: Loss=2.0026, Accuracy=28.41%, Backdoor SR=38.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:56:33,681 - INFO - Round 1: Loss=1.8642, Accuracy=33.29%, Backdoor SR=26.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:58:50,314 - INFO - Round 2: Loss=1.7714, Accuracy=36.87%, Backdoor SR=21.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 10:01:08,169 - INFO - Round 3: Loss=1.6945, Accuracy=38.92%, Backdoor SR=21.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 10:03:23,604 - INFO - Round 4: Loss=1.6318, Accuracy=41.12%, Backdoor SR=20.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 10:05:41,944 - INFO - Round 5: Loss=1.5814, Accuracy=43.12%, Backdoor SR=18.47%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 10:07:59,699 - INFO - Round 6: Loss=1.5439, Accuracy=44.25%, Backdoor SR=19.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n",
      "Client 2: Malicious\n",
      "Client 3: Benign\n",
      "Client 4: Benign\n",
      "Client 5: Benign\n",
      "Client 6: Benign\n",
      "Client 7: Benign\n",
      "Client 8: Benign\n",
      "Client 9: Benign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 10:10:22,006 - INFO - Round 7: Loss=1.5126, Accuracy=45.27%, Backdoor SR=17.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: Malicious\n",
      "Client 1: Malicious\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 255\u001b[0m\n\u001b[1;32m    252\u001b[0m         run_experiment(num_clients, percent, logger, use_defense\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 255\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 252\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m run_experiment(num_clients, percent, logger, use_defense\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    251\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercent\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% malicious clients (Defense ON)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_defense\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 238\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(num_clients, malicious_percent, logger, target_label, num_rounds, use_defense)\u001b[0m\n\u001b[1;32m    236\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mround\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_rounds):\n\u001b[0;32m--> 238\u001b[0m     loss, acc, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     backdoor_sr \u001b[38;5;241m=\u001b[39m server\u001b[38;5;241m.\u001b[39mevaluate_backdoor(target_label) \u001b[38;5;28;01mif\u001b[39;00m num_malicious \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    240\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Backdoor SR=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackdoor_sr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 175\u001b[0m, in \u001b[0;36mServer.train_round\u001b[0;34m(self, local_epochs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients:\n\u001b[1;32m    174\u001b[0m     set_model_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, global_params)\n\u001b[0;32m--> 175\u001b[0m     client_updates\u001b[38;5;241m.\u001b[39mappend(\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    176\u001b[0m aggregated, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_updates(client_updates)\n\u001b[1;32m    177\u001b[0m set_model_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, aggregated)\n",
      "Cell \u001b[0;32mIn[5], line 94\u001b[0m, in \u001b[0;36mClient.train\u001b[0;34m(self, model, epochs, batch_size)\u001b[0m\n\u001b[1;32m     92\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     93\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target)\n\u001b[0;32m---> 94\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/envs/fedd2/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/envs/fedd2/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/envs/fedd2/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the CNN model\n",
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def get_model_params(model):\n",
    "    return {name: param.clone() for name, param in model.state_dict().items()}\n",
    "\n",
    "def set_model_params(model, params):\n",
    "    model.load_state_dict(params)\n",
    "\n",
    "# Client class with fixes\n",
    "class Client:\n",
    "    def __init__(self, client_id, train_data, test_data, is_malicious=False, target_label=None, attack_probability=1):\n",
    "        self.client_id = client_id\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.is_malicious = is_malicious\n",
    "        self.target_label = target_label\n",
    "        self.attack_probability = attack_probability\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def add_backdoor_trigger(self, image):\n",
    "        img = deepcopy(image)\n",
    "        h, w = img.shape[1:]\n",
    "        x = np.random.randint(2, w-3)\n",
    "        y = np.random.randint(2, h-3)\n",
    "        trigger_w = np.random.randint(2, 4)\n",
    "        trigger_h = np.random.randint(2, 4)\n",
    "        img[0, y:y+trigger_h, x-trigger_w:x+trigger_w] = 1.0\n",
    "        img[0, y-trigger_w:y+trigger_w, x:x+trigger_h] = 1.0\n",
    "        return img\n",
    "\n",
    "    def poison_dataset(self):\n",
    "        if not self.is_malicious:\n",
    "            return self.train_data\n",
    "        \n",
    "        poisoned_images = []\n",
    "        poisoned_labels = []\n",
    "        for img, label in self.train_data:\n",
    "            if np.random.random() < 0.2:  # Reduced poisoning rate to 20%\n",
    "                img = self.add_backdoor_trigger(img)\n",
    "                label = self.target_label\n",
    "            poisoned_images.append(img)\n",
    "            poisoned_labels.append(label)\n",
    "        return TensorDataset(torch.stack(poisoned_images), torch.tensor(poisoned_labels))\n",
    "\n",
    "    def train(self, model, epochs=1, batch_size=32):\n",
    "        model.train()\n",
    "        model.to(self.device)\n",
    "        attack_this_round = self.is_malicious and np.random.rand() < self.attack_probability\n",
    "        train_data = self.poison_dataset() if attack_this_round else self.train_data\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Lower learning rate\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "                optimizer.step()\n",
    "\n",
    "        print(f\"Client {self.client_id}: {'Malicious' if attack_this_round else 'Benign'}\")\n",
    "        return get_model_params(model)\n",
    "\n",
    "    def test(self, model):\n",
    "        model.eval()\n",
    "        model.to(self.device)\n",
    "        test_loader = DataLoader(self.test_data, batch_size=32, shuffle=False)\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "        return test_loss / len(test_loader.dataset), 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "# Server class with defense\n",
    "class Server:\n",
    "    def __init__(self, model, clients, use_defense=True, similarity_threshold=0.9):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.use_defense = use_defense\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.malicious_client_ids = {c.client_id for c in clients if c.is_malicious}\n",
    "\n",
    "    def calculate_detection_metrics(self, detected_clients):\n",
    "        tp = len(detected_clients & self.malicious_client_ids)\n",
    "        fp = len(detected_clients - self.malicious_client_ids)\n",
    "        fn = len(self.malicious_client_ids - detected_clients)\n",
    "        tn = len({c.client_id for c in self.clients}) - tp - fp - fn\n",
    "        return {\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn,\n",
    "            'true_negatives': tn,\n",
    "            'detection_rate': tp / len(self.malicious_client_ids) if self.malicious_client_ids else 0,\n",
    "            'false_positive_rate': fp / (fp + tn) if (fp + tn) else 0,\n",
    "            'precision': tp / len(detected_clients) if detected_clients else 0,\n",
    "            'total_malicious': len(self.malicious_client_ids),\n",
    "            'total_detected': len(detected_clients),\n",
    "            'detected_clients': detected_clients,\n",
    "            'actual_malicious': self.malicious_client_ids\n",
    "        }\n",
    "\n",
    "    def fools_gold_defense(self, updates):\n",
    "        if not self.use_defense:\n",
    "            return {i: 1.0/len(updates) for i in range(len(updates))}, None\n",
    "        update_vectors = [np.concatenate([p.cpu().numpy().flatten() for p in u.values()]) for u in updates]\n",
    "        similarities = cosine_similarity(update_vectors)\n",
    "        weights = np.ones(len(updates))\n",
    "        detected = set()\n",
    "        for i in range(len(updates)):\n",
    "            for j in range(len(updates)):\n",
    "                if i != j and similarities[i][j] > self.similarity_threshold:\n",
    "                    weights[i] *= 0.5\n",
    "                    detected.add(self.clients[i].client_id)\n",
    "        if weights.sum() > 0:\n",
    "            weights /= weights.sum()\n",
    "        else:\n",
    "            weights = np.ones(len(updates)) / len(updates)\n",
    "        return {i: w for i, w in enumerate(weights)}, self.calculate_detection_metrics(detected)\n",
    "\n",
    "    def aggregate_updates(self, client_updates):\n",
    "        weights, metrics = self.fools_gold_defense(client_updates)\n",
    "        aggregated = {}\n",
    "        for name in client_updates[0].keys():\n",
    "            aggregated[name] = sum(upd[name] * weights[i] for i, upd in enumerate(client_updates))\n",
    "        return aggregated, metrics\n",
    "\n",
    "    def train_round(self, local_epochs=5):\n",
    "        global_params = get_model_params(self.model)\n",
    "        client_updates = []\n",
    "        for client in self.clients:\n",
    "            set_model_params(self.model, global_params)\n",
    "            client_updates.append(client.train(self.model, local_epochs))\n",
    "        aggregated, metrics = self.aggregate_updates(client_updates)\n",
    "        set_model_params(self.model, aggregated)\n",
    "        loss, accuracy = self.evaluate()\n",
    "        return loss, accuracy, metrics\n",
    "\n",
    "    def evaluate(self):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        for client in self.clients:\n",
    "            loss, acc = client.test(self.model)\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        return total_loss / len(self.clients), total_acc / len(self.clients)\n",
    "\n",
    "    def evaluate_backdoor(self, target_label):\n",
    "        success, total = 0, 0\n",
    "        self.model.eval(import torch.optim as optim)\n",
    "        for client in self.clients:\n",
    "            if client.is_malicious:\n",
    "                for data, _ in client.test_data:\n",
    "                    poisoned = client.add_backdoor_trigger(data.clone())\n",
    "                    poisoned = poisoned.unsqueeze(0).to(self.device)\n",
    "                    pred = self.model(poisoned).argmax(dim=1)\n",
    "                    success += (pred == target_label).item()\n",
    "                    total += 1\n",
    "        return 100.0 * success / total if total else 0.0\n",
    "\n",
    "# Experiment setup and execution\n",
    "def setup_logging():\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = f'logs/federated_training_{timestamp}.log'\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def load_and_split_data(num_clients):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    train_set = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "    test_set = datasets.CIFAR10('./data', train=False, transform=transform)\n",
    "    client_trains = random_split(train_set, [len(train_set)//num_clients]*num_clients)\n",
    "    client_tests = random_split(test_set, [len(test_set)//num_clients]*num_clients)\n",
    "    return client_trains, client_tests\n",
    "\n",
    "def run_experiment(num_clients, malicious_percent, logger, target_label=7, num_rounds=10, use_defense=True):\n",
    "    logger.info(f\"Starting experiment with {malicious_percent*100}% malicious clients\")\n",
    "    train_splits, test_splits = load_and_split_data(num_clients)\n",
    "    num_malicious = int(num_clients * malicious_percent)\n",
    "    clients = []\n",
    "    for i in range(num_clients):\n",
    "        is_mal = i < num_malicious\n",
    "        clients.append(Client(i, train_splits[i], test_splits[i], is_mal, target_label if is_mal else None))\n",
    "    \n",
    "    server = Server(CIFAR10CNN(), clients, use_defense=use_defense)\n",
    "    results = []\n",
    "    for round in range(num_rounds):\n",
    "        loss, acc, metrics = server.train_round()\n",
    "        backdoor_sr = server.evaluate_backdoor(target_label) if num_malicious else 0\n",
    "        logger.info(f\"Round {round}: Loss={loss:.4f}, Accuracy={acc:.2f}%, Backdoor SR={backdoor_sr:.2f}%\")\n",
    "        results.append({'round': round, 'loss': loss, 'accuracy': acc, 'backdoor_sr': backdoor_sr})\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    logger = setup_logging()\n",
    "    num_clients = 10\n",
    "    percents = [0.3, 0.4, 0.5]\n",
    "    for percent in percents:\n",
    "        logger.info(f\"\\nRunning with {percent*100}% malicious clients (Defense OFF)\")\n",
    "        run_experiment(num_clients, percent, logger, use_defense=False)\n",
    "        logger.info(f\"\\nRunning with {percent*100}% malicious clients (Defense ON)\")\n",
    "        run_experiment(num_clients, percent, logger, use_defense=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
